---
layout: post
title: epoll, poll, select的底层解析
category: 系统
tags: io
description: io多路复用底层解密
---

## IO 多路复用简介

首先说一下 IO 多路复用，通俗的讲它就是在一个线程中处理多个 IO 的读写。首先说一下，其实 IO 多路复用是为了解决高并发网络连接的问题，所以我们这里讨论的都是网咯 Socket 的 IO 读写问题，文件的读写不存在同样的问题。好了，现在想一下我们使用传统的 blocking io 来进行多路复用会有哪些问题？首先假设我们现在有 10 个连接，我们不知道哪个里面有数据，假设我们对第一个进行读，如果此时这个连接没有接受到数据， 就会将线程阻塞到这里。而这时，其他的九个连接就算有数据接收到了，我们也根本无法进行处理。所以，我们不能阻塞到这里，如果没有数据，就立即返回，有多少数据就读多少数据。这就是 non-blocking io 了，好了，想一下，现在我们可以这样来写我们的应用程序。

```java
for (;;) {
    for (SocketChannel sc : sces) {
        int r = sc.read(readBuf);
        //进行处理
        sc.write(sendBuf);
    }
}
```

但是这样的应用程序有个问题，就是如果此时连接没有数据，很有可能程序的大量时间就花费在了空轮询上。这很显然是低效率的，所以我们需要一种机制，在没有数据时，阻塞睡眠当前程序，同时当连接接受到数据时，唤醒我们的程序来处理。这在 Java 中就是 Select, 我们将 SocketChannel 注册到上面，就可以使用它来阻塞，等到有连接接收到数据在唤醒。

```java
Selector selector = Selector.open();
//将channel 注册到selector上
...

for (;;) {
    selector.select();
    Set<SelectionKey> keys = selector.selectedKeys();
    for (SelectionKey key : keys) {
       Channel ch = key.channel();
       if (key.isAccept()) {
           //处理接受请求
       }
       if (key.isReadable()) {
           //读取数据
       }
    }
}
```

那么 Selector 在底层是如何实现呢，这个根据平台有所差异，在 linux 上一般有三种实现，即 select、poll、epoll。这三个底层都是使用 file 的 poll 机制，不过实现上略有不同。

## linux 中的文件 poll 机制

首先，先看一下 linux 中的 file 结构，系统每个打开的文件都在内核空间中关联一个 file 结构。定义如下。

```c
struct file {

　　union {

　　      struct list_head fu_list; //文件对象链表指针linux/include/linux/list.h

　　      struct rcu_head fu_rcuhead; //RCU(Read-Copy Update)是Linux 2.6内核中新的锁机制

　　 } f_u;
　　struct path f_path; //包含dentry和mnt两个成员，用于确定文件路径

　　const struct file_operations *f_op; //与该文件相关联的操作函数

　　atomic_t f_count; //文件的引用计数(有多少进程打开该文件)

    ....

};
```

这个结构体中包含很多信息，不过我们现在不用管那么多，只需要知道里面有个 file_operations 结构，封装着与文件相关的操作函数，实际上也就是设备驱动提供的驱动函数。

```c
struct file_operations {
　　struct module *owner;//拥有该结构的模块的指针，一般为THIS_MODULES
   loff_t (*llseek) (struct file *, loff_t, int);//用来修改文件当前的读写位置
   ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);//从设备中同步读取数据
   ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);//向设备发送数据
   ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t);//初始化一个异步的读取操作
   ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t);//初始化一个异步的写入操作
　　int (*readdir) (struct file *, void *, filldir_t);//仅用于读取目录，对于设备文件，该字段为NULL
   unsigned int (*poll) (struct file *, struct poll_table_struct *); //轮询函数，判断目前是否可以进行非阻塞的读写或写入
　　int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long); //执行设备I/O控制命令
　　long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); //不使用BLK文件系统，将使用此种函数指针代替ioctl
　　long (*compat_ioctl) (struct file *, unsigned int, unsigned long); //在64位系统上，32位的ioctl调用将使用此函数指针代替
　　int (*mmap) (struct file *, struct vm_area_struct *); //用于请求将设备内存映射到进程地址空间
　　int (*open) (struct inode *, struct file *); //打开
　　int (*flush) (struct file *, fl_owner_t id);
　  ...
}
```

用户进程在对设备文件进行 read/write 读写的时候，系统其实就是寻找文件的 file 结构，然后通过其中 file_operations 中驱动提供的函数进行与设备进行交互的。而 file_operations 中函数的实现其实就是由底层驱动实现的。而我们注意到其中有一个 poll 函数，linux 下的 select、poll、epoll 都是调用这个函数来实现 io 多路复用的。好，那么我们看一下 poll 函数的大致实现。

```c
typedef void (*poll_queue_proc)(struct file *f, wait_queue_head_t *whead, struct poll_table_struct *pt);

typedef struct poll_table_struct {
    poll_queue_proc qproc;
} poll_table;

static unsigned int globalfifo_poll(struct file * filp, poll_table * wait)
 {
      unsigned int mask =0;
      struct globalfifo_dev *dev = filp->private_data;

      mutex_lock(&dev->mutex);

      poll_wait(filp, &dev->wait, wait);//将当前进程加入到写等待队列

     if(dev->current_len != 0)
         mask |=POLLIN | POLLRDNORM;//当有数据时，报告可读状态
     if(dev->current_len != GLOBALMEM_SIZE)
         mask |=POLLOUT | POLLWRNORM;//当缓冲区未满时，报告可写状态

     mutex_unlock(&dev->mutex);
     return mask;
 }

 struct __wait_queue_head {
 	spinlock_t lock;
 	struct list_head task_list;
 };
 typedef struct __wait_queue_head wait_queue_head_t;

 struct __wait_queue {
 	unsigned int flags;
 	void *private;
 	wait_queue_func_t func;
 	struct list_head task_list;
 }

 typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int sync, void *key);

 #define __WAIT_QUEUE_INITIALIZER(name) {         \
      .lock = __SPIN_LOCK_UNLOCK(name.lock),// lock = 1;  \
      .task_list = {&(name).task_list, &(name).task_list} }

#define DECLARE WAIT_QUEUE_HEAD(name) \
      wait_queue_head_t name = __WAIT_QUEUE_INITIALIZER(name)

static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p) {
    if (p && wait_address)
        p->qproc(filp, wait_address, p);
}
```

我们可以看到，我们调用 file_operations 中的 poll 函数，将文件结构和我们的一个回调函数传入，之后，poll 中会回调我们传入的函数。而一般我们传入的回调函数， 都是根据传入的文件，等待队列，创建当前进场的一个等待项，这样当设备收到数据的时候就会唤醒当前进程（唤醒方式是由其中的 func 决定的）。

## Select 的实现

首先看一下 slect 函数的定义

```c
int select(int maxfdp,fd_set *readfds,fd_set *writefds,fd_set *errorfds,struct timeval *timeout);
```

fd_set 其实是一个位图，用位的方式来记录文件描述符。

第一个参数为轮询的文件描述符总数。

第二个参数是监视读的文件描述符集合

第三个参数是监视写的文件描述符集合

第四个参数是监视异常的文件描述符集合

第五个是等待时间

返回值：负值错误，正值：有事件的文件总数。

调用过程：

```
select
	|
	转到内核态
	|
	sys_select
		|
		处理时间
		|
		core_sys_select
    		|
    		将fd_set转入内核空间
            |
    		do_select
    			|
    			for(;;) {
    				遍历文件描述符，调用poll查看文件状态
    				如果有监视文件的响应事件产生或时间溢出，跳出循环，返回对应fd_set
    				否则根据timeout休眠等待唤醒
                }

socket -> 事件产生 -> 唤醒select线程
```

## Poll 的实现

同样先看一下 poll 函数定义

```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

和 select 相比改用数组来传递监听的文件集合，没有了 1024 的限制，后面的时间和返回值都一样。

调用过程：

```c
poll
	|
	转到内核态
	|
	sys_poll
		|
		处理时间
		|
		do_sys_poll
			|
			将pollfd拷贝到内核空间
			初始化poll_wqueues
			|
			do_poll
				|
				for (;;) {
					for(walk = list; walk != nullptr; walk=walk->next) {
						for (pfd : walk->entries) {
							do_pollfd
								|
								file->f_op->poll(file, pwait)
						}
					}
				}


socket->事件产生-> 将对应pfd加入到list中
```

## epoll 的实现

与前两个不同，epoll 提供了三个函数来使用。

```c
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
```

使用起来很清晰，首先要调用 epoll_create 建立一个 epoll 对象。参数 size 是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。epoll_ctl 可以操作上面建立的 epoll，例如，将刚建立的 socket 加入到 epoll 中让其监控，或者把 epoll 正在监控的某个 socket 句柄移出 epoll，不再监控它等等。

从上面的调用方式就可以看到 epoll 比 select/poll 的优越之处：因为后者每次调用时都要传递你所要监控的所有 socket 给 select/poll 系统调用，这意味着需要将用户态的 socket 列表 copy 到内核态，如果以万计的句柄会导致每次都要 copy 几十几百 KB 的内存到内核态，非常低效。而我们调用 epoll_wait 时就相当于以往调用 select/poll，但是这时却不用传递 socket 句柄给内核，因为内核已经在 epoll_ctl 中拿到了要监控的句柄列表。

所以，实际上在你调用 epoll_create 后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用 epoll_ctl 只是在往内核的数据结构里塞入新的 socket 句柄。

在看 epoll_wait 的调用过程之前，我们先看一下结构定义

```c
struct eventpoll {
    /* Protect the this structure access */
    spinlock_t lock;

    /*
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     */
    struct mutex mtx;

    /* Wait queue used by sys_epoll_wait() */
    /* 阻塞在epoll_wait()当前epoll实例的用户被链接到这个等待队列 */
    wait_queue_head_t wq;

    /* Wait queue used by file->poll() */
    /* epoll文件也可以被epoll_wait() */
    wait_queue_head_t poll_wait;

    /* List of ready file descriptors */
    /* 已经ready的epitem的链表 */
    struct list_head rdllist;

    /* RB tree root used to store monitored fd structs */
    /* 存储epitem */
    struct rb_root rbr;

    /*
     * This is a single linked list that chains all the "struct epitem" that
     * happened while transfering ready events to userspace w/out
     * holding ->lock.
     */
    /* 见ep_poll_callback()以及ep_scan_ready_list()中的注释 */
    struct epitem *ovflist;

    /* The user that created the eventpoll descriptor */
    /* 创建当前epoll实例的用户 */
    struct user_struct *user;
};

/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the "rbr" RB tree.
 */
struct epitem {
    /* RB tree node used to link this structure to the eventpoll RB tree */
    /* eventpoll内部的红黑树的挂载点 */
    struct rb_node rbn;

    /* List header used to link this structure to the eventpoll ready list */
    /* 所有已经ready的epitem都会被挂载到eventpoll的rdllist中 */
    struct list_head rdllink;

    /*
     * Works together "struct eventpoll"->ovflist in keeping the
     * single linked chain of items.
     */
    /* 配合eventpoll->ovflist使用 */
    struct epitem *next;

    /* The file descriptor information this item refers to */
    /*
     * 作为evetnpoll内部的红黑树节点的key
     */
    struct epoll_filefd ffd;

    /* Number of active wait queue attached to poll operations */
    /* 监听队列挂载数 */
    /* 难道一个epitem还能同时挂载到多个监听队列？ */
    int nwait;

    /* List containing poll wait queues */
    /* 链接当前epitem对应的eppoll_entry结构 */
    struct list_head pwqlist;

    /* The "container" of this item */
    /* 关联当前epitem所属的epollevent */
    struct eventpoll *ep;

    /* List header used to link this item to the "struct file" items list */
    /* 与所监听的struct file进行链接 */
    struct list_head fllink;

    /* The structure that describe the interested events and the source fd */
    /* 通过epoll_ctl从用户空间传过来的数据，表示当前epitem关心的events */
    struct epoll_event event;
};

struct epoll_filefd {
    struct file *file;
    int fd;
};

struct epoll_event {
    __u32 events;
    __u64 data;
};

/* Wrapper struct used by poll queueing */
struct ep_pqueue {
    poll_table pt;
    struct epitem *epi;
};

/*
 * structures and helpers for f_op->poll implementations
 */
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);

typedef struct poll_table_struct {
    poll_queue_proc qproc;
    unsigned long key;
} poll_table;

/* Wait structure used by the poll hooks */
/* 挂载到资源文件监听队列中的钩子结构 */
struct eppoll_entry {
    /* List header used to link this structure to the "struct epitem" */
    /* 与其关联的epitem进行链接 */
    struct list_head llink;

    /* The "base" pointer is set to the container "struct epitem" */
    /* 指向对应的epitem结构 */
    /*
     * 既然llink字段已经与对应的epitem结构进行了链接，为什么还需要
     * 一个base指针指向对应的epitem？？？
     */
    struct epitem *base;

    /*
     * Wait queue item that will be linked to the target file wait
     * queue head.
     */
    /* 挂载到资源文件监听队列的节点 */
    wait_queue_t wait;

    /* The wait queue head that linked the "wait" wait queue item */
    /* 资源监听队列队列头 */
    wait_queue_head_t *whead;
};

/* Used by the ep_send_events() function as callback private data */
struct ep_send_events_data {
    int maxevents;
    struct epoll_event __user *events;
};
```

好现在看一下 epoll_wait 的调用过程。

```c
epoll_wait
	|
	进入内核态
	|
	sys_epoll_wait
		|
		将数据转到内核空间
		|
		ep_poll
			|
			转换时间
			|
			for (;;) {
				set_current_state(TASK_INTERRUPTIBLE);
            	/* events就绪或者超时，跳出循环 */
            	if (!list_empty(&ep->rdllist) || !jtimeout)
              	  break;
           	 /* 出现未决信号，设置返回值为-EINTR并跳出循环 */
           	 if (signal_pending(current)) {
               	 	res = -EINTR;
                	break;
            	}

            	spin_unlock_irqrestore(&ep->lock, flags);
            	/* 休眠...等待超时或者被就绪资源唤醒 */
            	jtimeout = schedule_timeout(jtimeout);
            	spin_lock_irqsave(&ep->lock, flags);
			}


socket -> 事件发生 -> 调用回调将fd加入到就绪列表->唤醒进程
```

这里我们可以发现，好像 epoll 跟 poll 和 select 不同，并没有在这里调用 poll 函数，而是判断就绪列表，因为 poll 函数已经在 epoll_ctx 加入的时候调用了，并且注册了回调，在事件发生后，将对应的文件加入到就绪列表中，所以这里就不用轮询了。

## epoll、poll 和 select 的对比

好下面我们来做一个对比吧。

**select**

使用位图保存文件集合，32 位最大 1024, 64 位最大 2048

遍历调用设备的 poll 查询状态，并加入等待队列，有时间产生时唤醒进程

遍历一遍后休眠等待被再次唤醒

缺点：效率低，有数量限制

优点：跨平台

**poll**

使用数组文件集合，无数量限制

遍历调用设备的 poll 查询状态，并加入等待队列，有时间产生时唤醒进程

遍历一遍后休眠等待被再次唤醒

缺点：效率低

优点：没有数量限制

**epoll**

尽在注册时将数据转入内核空间，并使用 poll 查询注册回调，使用红黑树保存文件集合，回调查询红黑树，将就绪的文件加入就绪列表，epoll_wait 时仅仅检查就绪列表，效率更高。并且支持 ET 和 LT 两种触发模式。

优点：更适用于大量连接，并且不活跃的情况，比如 http 服务器。

缺点：在连接非常活跃情况可能会低效；不跨平台。
